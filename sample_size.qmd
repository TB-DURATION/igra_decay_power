---
title: "IGRA+ decay sample size calculation"
format: html
editor: source
toc: true
number-sections: true
echo: true
code-fold: false
warning: false
editor_options: 
  chunk_output_type: console
---

```{r setup, include = FALSE}
knitr::knit_hooks$set(
  margin = function(before, options, envir) {
    if (before) par(mgp = c(1.5, .5, 0)) #, bty = "n", plt = c(.105, .97, .13, .97))
    else NULL
  })

knitr::opts_chunk$set(echo       = TRUE,
                      message    = FALSE,
                      cache      = TRUE,
                      autodep    = TRUE,
#                      dev.args   = list(pointsize = 11),
                      fig.height = 5,
                      fig.width  = 1.4 * 5,
                      fig.retina = 2,
                      fig.align  = "center",
                      margin     = TRUE 
)

setHook("plot.new", function() par(mgp = c(1.5, .5, 0), bty = "n"), "prepend")
```

All the data and source code files are [here](https://github.com/TB-DURATION/igra_decay_power).

You can ask report errors or for additional analysis [here](https://github.com/TB-DURATION/igra_decay_power).

## Packages

Needed packages: 

```{r}
needed_packages <- c(
  "dplyr",
  "bbmle",
  "purrr",
  "magrittr",
  "tidyr"
)
```

Loading packages:

```{r}
library(dplyr)
library(bbmle)
library(purrr)
library(magrittr)
```

## Utility functions

Tuning `abline()`

```{r}
abline2 <- function(...) abline(..., col = "lightgrey")
```

The logit function:

```{r}
logit <- function(x) log(x / (1 - x))
```

The logistic function:

```{r}
logistic <- function(x) 1 / (1 + exp(- x))
```

Random draws following the Bernoulli law:

```{r}
rbern <- function(p) rbinom(length(p), 1, p)
```

A tuning of the `runif()` function:

```{r}
runif2 <- function(max_t_before) runif(length(max_t_before), 0, max_t_before)
```

A function that adds to a dataframe colones of proportion estimates together with 
borders of a confidence interval from a column `x` of successes and a column `n` of
trials. This is a wrapper around the `prop.test()` function.

```{r}
add_prop_est <- function(df, x, n, p = "est", l = "lwr", u = "upr", ...) {
  df |> 
    mutate(test   = map2({{ x }}, {{ n }}, prop.test, ...),
           "{p}" := map_dbl(test, ~ .x[["estimate"]]),
           conf   = map(test, ~ setNames(.x[["conf.int"]], c(l, u)))) |> 
    tidyr::unnest_wider(conf) |> 
    select(- test)
}
```

## A simulator

A function that simulates data according to an exponential decay:

```{r}
simulator <- function(p0, p1, t, t_entry, t_exposure = 0) {
  tibble(time = t_entry,
         igra = rbern(p0 * exp(log(p1 / p0) * (t_entry + t_exposure) / t)))
}
```

Where `p0` is the assumed initial proportion of IGRA+, `p1` is the assumed proportion of
IGRA+ `t` years later initial time, `t_entry` is a vector of sampling times (i.e. dates
of entry in the country) and `t_exposure` is a vector of the time before entry at which 
exposure happened. Note that:

* both `t_entry` and `t_exposure` are vectors of random values
* the number of samples corresponds to the length of `t_entry`
* the length of `t_exposure` is either equal to 1 or the number of samples

An example with 500 samples showing what the simulations look like:

```{r}
simulator(.3, .1, 10, runif(500, max = 10))
```

To explore a bit the simulations, let's create a function that ease the vizualization
of the Bernoulli trials data:

```{r}
show_simulations <- function(simulations, xlim = c(0, 10), breaks = 0:10) {
  simdata <- simulations |> 
    mutate(time2 = cut(time, breaks)) |> 
    group_by(time2) |> 
    summarise(x = sum(igra), n = length(igra)) |> 
    add_prop_est(x, n) |> 
    mutate_at(c("est", "lwr", "upr"), multiply_by, 100) |> 
    mutate(time = as.integer(time2) - .5)

  plot(NA, xlim = xlim, ylim = c(0, max(simdata$upr)),
       xlab = "time since entry (years)", ylab = "% IGRA+")
  abline2(h = 10 * 0:10); abline2(v = 0:max(xlim))
  
  with(simdata, {
    points(time, est)
    arrows(time, lwr, time, upr, .1, 90, 3)
  })
}
```

And let's build the following wrapper around this function:

```{r}
explore_simulations <- function(p0 = .3, p1 = .1, t = 10, n = 1000,
                                xlim = c(0, 10), breaks = 0:10) {
  show_simulations(simulator(p0, p1, t, runif(n, max = t)), xlim, breaks)
  xs <- seq(0, t, le = 500)
  lines(xs, 100 * p0 * exp(xs * log(p1 / p0) / t))
}
```

Let's play with it:

```{r}
explore_simulations(p0 = .3, p1 = .1, t = 10, n = 10000)
```

## Exponential decay model

### ML estimation

Let's write down the minus log-likelihood of the exponential decay model:

```{r}
mLL <- function(logita, logmlambda, time, igra) {
  p <- logistic(logita) * exp(- exp(logmlambda) * time)
  igra <- as.logical(igra)
  - sum(log(c(p[igra], (1 - p)[! igra])))
}
```

Where `logitsa` is the logit value of the initial proportion of IGRA+, `logmlambda` is
the log of minus the rate of decay and `time` and `igra` are the columns of the
simulation dataframe. Let's generate some simulations:

```{r}
simulations <- simulator(.3, .1, 10, runif(500, max = 10))
```

And let's use these simulated data to compute the likelihood of the exponential decay
model:

```{r}
mLL(logit(.3), log(- log(.1 / .3) / 10), simulations$time, simulations$igra)
```

### Power calculation

A function that computes the ability, on simulations data set, to reject H0 according to
which the decay rate is equal to 0, with the type-I risk `alpha`:

```{r}
decay_rate_test <- function(simulations, p0, p1, t, alpha = .05) {
  mle3 <- function(...) mle2(mLL, data = simulations, ...)
  model0 <- mle3(start = list(logita = logit(p0)), fixed = list(logmlambda = - Inf))
  model1 <- mle3(start = list(logita = logit(p0), logmlambda = log(log(p0 / p1) / t)))
  anova(model0, model1)[2, 5] < alpha
}
```

Let's try it:

```{r}
decay_rate_test(simulations, .3, .1, 10)
```

A wrapper around the combination of `simulator()` and `decay_rate_test()`, allowing to
repeat the operation `N` times:

```{r}
decay_rate_power <- function(p0, p1, t, t_entry, t_exposure = 0, alpha = .05, N = 100) {
  replicate(N, decay_rate_test(simulator(p0, p1, t, t_entry, t_exposure),
                               p0, p1, t, alpha))
}
```

Let's try it:

```{r}
out <- decay_rate_power(p0 = .3, p1 = .1, t = 10, t_entry = runif(200, max = 10),
                        N = 1000)
```

And see whether the number of simulations (1000) looks about OK (i.e. whether the power
has converged):

```{r}
plot(cummean(out),
     type = "l", xlab = "number of simulations", ylab = "power", ylim = 0:1)
```

Looks about OK.